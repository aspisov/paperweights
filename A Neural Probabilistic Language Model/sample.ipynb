{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nplm.utils import load_model\n",
    "from nplm.data_setup import load_vocab\n",
    "from nplm.model import Config, NeuralProbabilisticLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_vocab(file_path=\"data/vocab.pkl\")\n",
    "\n",
    "# hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "context_size = 5\n",
    "hidden_size = 100\n",
    "embed_size = 60\n",
    "direct = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 6.59M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralProbabilisticLanguageModel(\n",
       "  (C): Embedding(14222, 60)\n",
       "  (H): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (U): Linear(in_features=100, out_features=14222, bias=True)\n",
       "  (W): Linear(in_features=300, out_features=14222, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config(vocab_size=vocab_size, embed_size=embed_size, hidden_size=hidden_size, context_size=context_size, direct=direct)\n",
    "\n",
    "model = NeuralProbabilisticLanguageModel(config)\n",
    "\n",
    "file_name = f\"model_n{context_size}_h{hidden_size}_m{embed_size}\"\n",
    "if direct:\n",
    "    file_name += \"_direct\"\n",
    "load_model(model, file_name=file_name + \".pth\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, vocab, idx_to_word, initial_context, context_size=10, steps=50, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text using a pre-trained language model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The pre-trained language model.\n",
    "    - vocab: A dictionary mapping word to index used for the model's vocabulary.\n",
    "    - idx_to_word: A dictionary mapping index to word, used to convert model outputs to text.\n",
    "    - initial_context: A string or list of words to start text generation.\n",
    "    - context_size: The size of the context window expected by the model.\n",
    "    - steps: Number of additional words to generate.\n",
    "    - temperature: A factor to control the randomness of predictions by scaling the logits before applying softmax.\n",
    "\n",
    "    Returns:\n",
    "    - A string containing the original context extended by the generated words.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    if isinstance(initial_context, str):\n",
    "        initial_context = initial_context.split()  # Split initial context string into words\n",
    "\n",
    "    # Map words to their indices\n",
    "    context_indices = [vocab.get(word, vocab[\"<UNK>\"]) for word in initial_context]\n",
    "    \n",
    "    # Pad the context if it is shorter than required\n",
    "    if len(context_indices) < context_size:\n",
    "        padding = [vocab.get(\"\", vocab[\"<UNK>\"])] * (context_size - len(context_indices))\n",
    "        context_indices = padding + context_indices\n",
    "\n",
    "    context_tensor = torch.tensor([context_indices], dtype=torch.long).to(next(model.parameters()).device)\n",
    "    output_words = list(initial_context)  # Start with the initial context\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(steps):\n",
    "            logits = model(context_tensor[:, -context_size:])\n",
    "            # Use temperature to scale the logits and apply softmax to get probabilities\n",
    "            probabilities = F.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
    "            next_word = idx_to_word[next_token_idx]\n",
    "\n",
    "            # Update the context by sliding the window and including the new word\n",
    "            output_words.append(next_word)\n",
    "            context_indices = context_indices[1:] + [next_token_idx]\n",
    "            context_tensor = torch.tensor([context_indices], dtype=torch.long).to(context_tensor.device)\n",
    "\n",
    "    return ' '.join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, vocab, idx_to_word, initial_context, steps=50, temperature=1.0, max_resample=5):\n",
    "    \"\"\"\n",
    "    Generate text using a language model, handling unknown words by resampling.\n",
    "    \n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The trained model.\n",
    "        vocab (dict): Vocabulary mapping of words to indices.\n",
    "        idx_to_word (dict): Reverse mapping of indices to words.\n",
    "        initial_context (str or list): Initial text to start generation.\n",
    "        context_size (int): The number of tokens the model expects as input.\n",
    "        steps (int): Number of tokens to generate.\n",
    "        temperature (float): Softmax temperature for generation.\n",
    "        max_resample (int): Maximum number of resampling attempts per step to avoid <UNK>.\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated text.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if isinstance(initial_context, str):\n",
    "        initial_context = initial_context.split()  # Split initial context string into words\n",
    "    \n",
    "    # Adjust initial context size\n",
    "    if len(initial_context) > context_size:\n",
    "        initial_context = initial_context[-context_size:]\n",
    "    elif len(initial_context) < context_size:\n",
    "        initial_context = [''] * (context_size - len(initial_context)) + initial_context\n",
    "\n",
    "    # Map words to their indices\n",
    "    context_indices = [vocab.get(word, vocab['<UNK>']) for word in initial_context]\n",
    "    context_tensor = torch.tensor([context_indices], dtype=torch.long).to(next(model.parameters()).device)\n",
    "    output_words = list(initial_context)\n",
    "    \n",
    "\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(steps):\n",
    "            logits = model(context_tensor)\n",
    "            # Use temperature to scale the logits and apply softmax to get probabilities\n",
    "            probabilities = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "            # resample <UNK>\n",
    "            for _ in range(max_resample):\n",
    "                next_token_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
    "                if next_token_idx != vocab['<UNK>'] or max_resample <= 1:\n",
    "                    break\n",
    "            \n",
    "            next_word = idx_to_word[next_token_idx]\n",
    "\n",
    "            # Update the context by sliding the window and including the new word\n",
    "            output_words.append(next_word)\n",
    "            context_indices = context_indices[1:] + [next_token_idx]\n",
    "            context_tensor = torch.tensor([context_indices], dtype=torch.long).to(context_tensor.device)\n",
    "\n",
    "    return ' '.join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Lisa is a suit , tested with resources , and it or a change he is sure i have not any time it is not to be on\n",
      "  Lisa is a rural formula , the development of the large , are of the international law , and that other physically below the surface . now where\n",
      "  Lisa is a toss always live in his lips . he had the answer to a sea of components for four years of the experiment in groups of\n",
      "  Lisa is a constant to me the dust that had on the one side . each card a remarkable spent one half of the time to the full\n",
      "  Lisa is a candidate who does not trade to whom , it was to be able to create a advice by dr. gordon a. and how his old\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Lisa is a\"\n",
    "idx_to_word = {v: k for k, v in vocab.items()}  # Create reverse mapping\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = sample(model, vocab, idx_to_word, prompt.split(), steps=25, temperature=0.8)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
