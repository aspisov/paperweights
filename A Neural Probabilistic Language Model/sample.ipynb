{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nplm.utils import load_model\n",
    "from nplm.data_setup import load_vocab\n",
    "from nplm.model import Config, NeuralProbabilisticLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_vocab(file_path=\"data/vocab.pkl\")\n",
    "\n",
    "# hyperparameters\n",
    "V = len(vocab)\n",
    "n = 5\n",
    "h = 50\n",
    "m = 120\n",
    "direct = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.46M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralProbabilisticLanguageModel(\n",
       "  (C): Embedding(14222, 120)\n",
       "  (H): Linear(in_features=600, out_features=50, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (U): Linear(in_features=50, out_features=14222, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config(vocab_size=V, embed_size=m, hidden_size=h, context_size=n, direct=direct)\n",
    "\n",
    "model = NeuralProbabilisticLanguageModel(config)\n",
    "\n",
    "file_name = f\"model_n{n}_h{h}_m{m}\"\n",
    "if direct:\n",
    "    file_name += \"_direct\"\n",
    "load_model(model, file_name=file_name + \".pth\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, vocab, idx_to_word, initial_context, steps=50, temperature=1.0):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    if isinstance(initial_context, str):\n",
    "        initial_context = initial_context.split()  # Split initial context string into words\n",
    "\n",
    "    # Map words to their indices\n",
    "    context_indices = [vocab.get(word, vocab[\"<UNK>\"]) for word in initial_context]\n",
    "    context_tensor = torch.tensor([context_indices], dtype=torch.long).to(next(model.parameters()).device)\n",
    "    output_words = list(initial_context)  # Start with the initial context\n",
    "\n",
    "    with torch.no_grad():  # We do not need to track gradients here\n",
    "        for _ in range(steps):\n",
    "            logits = model(context_tensor)\n",
    "            # Use temperature to scale the logits and apply softmax to get probabilities\n",
    "            probabilities = F.softmax(logits / temperature, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
    "            next_word = idx_to_word[next_token_idx]\n",
    "\n",
    "            # Update the context by sliding the window and including the new word\n",
    "            output_words.append(next_word)\n",
    "            context_indices = context_indices[1:] + [next_token_idx]\n",
    "            context_tensor = torch.tensor([context_indices], dtype=torch.long).to(context_tensor.device)\n",
    "\n",
    "    return ' '.join(output_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisa is a very nice voice , he says , `` a man has to make indeed , on the problem in their activities .\n",
      "Lisa is a very nice '' . he says , `` i met the <UNK> of the <UNK> . <UNK> <UNK> over a very long\n",
      "Lisa is a very nice night , and maybe you do for occur . brown passed a <UNK> paul <UNK> , the <UNK> <UNK> ,\n",
      "Lisa is a very nice broad staff . but clearly its chinese friends in the <UNK> of the company , the emory university most dying\n",
      "Lisa is a very nice change , and i have given to the tradition of his son , and that of the <UNK> sea ,\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Lisa is a very nice\"\n",
    "idx_to_word = {v: k for k, v in vocab.items()}  # Create reverse mapping\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = sample(model, vocab, idx_to_word, prompt.split(), steps=20, temperature=0.8)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
