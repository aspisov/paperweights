# GPT2

This folder contains a trained GPT2 model with BPE tokenization trained on Tiny-Stories

## Dataset

## Tokenization
Current implementation is using GPT4o regex expression for tokenization which is mostly optimized for working with 

# References
- [YSDA Lena Voita lecture](https://github.com/yandexdataschool/nlp_course/tree/2024/week04_seq2seq)
- [Attention is all you need paper](https://arxiv.org/abs/1706.03762)
- [GPT2 paper](https://openai.com/index/better-language-models/)
- [Andrej Karpathy tokenizer video](https://youtu.be/zduSFxRajkE?si=xwqQ4F-VT0zHoRvc)
