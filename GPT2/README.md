# GPT2

This folder contains a trained GPT2 model with BPE tokenization trained on Tiny-Stories

## Dataset

## Tokenization
In this work I'm very closely replication GPT2 tokenizer. It has a lot of 
disadvantages compared to other tokenizers when working with code. However I'm 
training my language model on Tiny-Stories dataset, thus it's almost perfect.

# References
- [YSDA Lena Voita lecture](https://github.com/yandexdataschool/nlp_course/tree/2024/week04_seq2seq)
- [Andrej Karpathy tokenizer video](https://youtu.be/zduSFxRajkE?si=xwqQ4F-VT0zHoRvc)
- [GPT2 paper](https://openai.com/index/better-language-models/)