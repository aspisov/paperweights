{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUaE6J3H1bBY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpbmvm1d1bBa"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "MAX_SEQ_LENGTH = 128\n",
        "\n",
        "EMBED_DIM = 256\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 3\n",
        "DROPOUT = 0.3\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD1AZKgr1bBa",
        "outputId": "eeb7838e-af87-4b9b-b205-e458f1be2294"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download(\"punkt\")\n",
        "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
        "\n",
        "train_lines = dataset['train']['text'][:200000]\n",
        "\n",
        "def preprocess(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "# tokenize\n",
        "train_lines = [preprocess(line) for line in train_lines]\n",
        "\n",
        "# building vocab\n",
        "word_counts = Counter([word for text in train_lines for word in text])\n",
        "vocab = (\n",
        "    [\"<UNK>\", \"<PAD>\", \"<EOS>\", \"<BOS>\"] +\n",
        "    [word for word, count in word_counts.items() if count > 10]\n",
        ")\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "PAD_idx = word_to_idx['<PAD>']\n",
        "UNK_idx = word_to_idx['<UNK>']\n",
        "EOS_idx = word_to_idx['<EOS>']\n",
        "BOS_idx = word_to_idx['<BOS>']\n",
        "\n",
        "indexed_data = [\n",
        "    [BOS_idx] + [word_to_idx.get(word, UNK_idx) for word in line[:MAX_SEQ_LENGTH-2]] + [EOS_idx]\n",
        "    for line in train_lines\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUxrmWi81bBb"
      },
      "outputs": [],
      "source": [
        "class TinyStoriesDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch.sort(key=lambda x: len(x), reverse=True)\n",
        "    sequences, lengths = zip(*[(torch.tensor(seq), len(seq)) for seq in batch])\n",
        "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=PAD_idx)\n",
        "    return sequences_padded, torch.tensor(lengths)\n",
        "\n",
        "dataset = TinyStoriesDataset(indexed_data)\n",
        "dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZlGNQ1I1bBc",
        "outputId": "4aa65958-7d26-4151-cb95-70895d1b52b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.81M parameters\n"
          ]
        }
      ],
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_dim,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "        print(f\"{self._count_parameters()/1e6:.2f}M parameters\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.dropout(self.embedding(x))\n",
        "        output, hidden = self.lstm(emb)\n",
        "        output = self.layer_norm(output)\n",
        "        logits = self.fc(output)\n",
        "        return logits\n",
        "\n",
        "    def _count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "model = LSTMLanguageModel(\n",
        "    len(vocab), EMBED_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT\n",
        ").to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_idx)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEL8ff8I9yMB"
      },
      "outputs": [],
      "source": [
        "def save_vocabulary(vocab, word_to_idx, save_dir):\n",
        "    vocab_path = os.path.join(save_dir, 'vocabulary.json')\n",
        "    vocab_data = {\n",
        "        'vocab': vocab,\n",
        "        'word_to_idx': word_to_idx\n",
        "    }\n",
        "    with open(vocab_path, 'w') as f:\n",
        "        json.dump(vocab_data, f)\n",
        "    print(f\"Vocabulary saved to {vocab_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waJHDGwS1bBc"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, criterion, optimizer, scheduler, num_epochs, save_dir='model_checkpoints'):\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    # save vocabulary at the start of training\n",
        "    save_vocabulary(vocab, word_to_idx, save_dir)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch, (sequence, lengths) in enumerate(dataloader):\n",
        "            sequence = sequence.to(device)\n",
        "\n",
        "            targets = sequence[:, 1:].contiguous()\n",
        "\n",
        "            outputs = model(sequence[:, :-1])\n",
        "\n",
        "            outputs = outputs.view(-1, outputs.size(2))\n",
        "            targets = targets.view(-1)\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch % 300 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "        scheduler.step(avg_loss)  # update learning rate\n",
        "\n",
        "        # save the model if it's the best so far\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': best_loss,\n",
        "            }, os.path.join(save_dir, 'best_model.pth'))\n",
        "            print(f'Model saved with loss: {best_loss:.4f}')\n",
        "\n",
        "        # save a checkpoint every epoch\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_loss,\n",
        "        }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7j32-m91bBc",
        "outputId": "33627f74-e5e5-43d0-978d-afea5311105d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary saved to model_checkpoints/vocabulary.json\n",
            "Epoch [1/3], Batch [1/3125], Loss: 2.3645\n",
            "Epoch [1/3], Batch [301/3125], Loss: 2.2720\n",
            "Epoch [1/3], Batch [601/3125], Loss: 2.2532\n",
            "Epoch [1/3], Batch [901/3125], Loss: 2.3526\n",
            "Epoch [1/3], Batch [1201/3125], Loss: 2.3472\n",
            "Epoch [1/3], Batch [1501/3125], Loss: 2.2434\n",
            "Epoch [1/3], Batch [1801/3125], Loss: 2.1862\n",
            "Epoch [1/3], Batch [2101/3125], Loss: 2.3557\n",
            "Epoch [1/3], Batch [2401/3125], Loss: 2.3001\n",
            "Epoch [1/3], Batch [2701/3125], Loss: 2.2825\n",
            "Epoch [1/3], Batch [3001/3125], Loss: 2.2108\n",
            "Epoch [1/3], Average Loss: 2.2630\n",
            "Model saved with loss: 2.2630\n",
            "Epoch [2/3], Batch [1/3125], Loss: 2.2328\n",
            "Epoch [2/3], Batch [301/3125], Loss: 2.0815\n",
            "Epoch [2/3], Batch [601/3125], Loss: 2.1563\n",
            "Epoch [2/3], Batch [901/3125], Loss: 2.1574\n",
            "Epoch [2/3], Batch [1201/3125], Loss: 2.1599\n",
            "Epoch [2/3], Batch [1501/3125], Loss: 2.1940\n",
            "Epoch [2/3], Batch [1801/3125], Loss: 2.0760\n",
            "Epoch [2/3], Batch [2101/3125], Loss: 2.1683\n",
            "Epoch [2/3], Batch [2401/3125], Loss: 2.2315\n",
            "Epoch [2/3], Batch [2701/3125], Loss: 2.1417\n",
            "Epoch [2/3], Batch [3001/3125], Loss: 2.1202\n",
            "Epoch [2/3], Average Loss: 2.1923\n",
            "Model saved with loss: 2.1923\n",
            "Epoch [3/3], Batch [1/3125], Loss: 2.1609\n",
            "Epoch [3/3], Batch [301/3125], Loss: 2.1107\n",
            "Epoch [3/3], Batch [601/3125], Loss: 2.1243\n",
            "Epoch [3/3], Batch [901/3125], Loss: 2.2224\n",
            "Epoch [3/3], Batch [1201/3125], Loss: 2.0360\n",
            "Epoch [3/3], Batch [1501/3125], Loss: 2.1005\n",
            "Epoch [3/3], Batch [1801/3125], Loss: 2.1294\n",
            "Epoch [3/3], Batch [2101/3125], Loss: 2.1063\n",
            "Epoch [3/3], Batch [2401/3125], Loss: 2.1241\n",
            "Epoch [3/3], Batch [2701/3125], Loss: 2.1180\n",
            "Epoch [3/3], Batch [3001/3125], Loss: 2.0474\n",
            "Epoch [3/3], Average Loss: 2.1472\n",
            "Model saved with loss: 2.1472\n"
          ]
        }
      ],
      "source": [
        "train(model, dataloader, criterion, optimizer, scheduler, num_epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhBRFN6z1bBc",
        "outputId": "93978bab-5d7b-41d0-8656-de449d04a591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "once upon a time , there was a man who was feeling very tired . they looked around and saw a huge , hairy dog . he was so happy when he was done , but then , something strange happened . in the end , but he did not notice it . it\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, start_sequence, max_length=100, temperature=0.8):\n",
        "    model.eval()\n",
        "    current_sequence = start_sequence\n",
        "    generated_sequence = start_sequence.copy()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for _ in range(max_length):\n",
        "            input_seq = torch.tensor(\n",
        "                [word_to_idx.get(word, UNK_idx) for word in current_sequence]\n",
        "            ).unsqueeze(0).to(device)\n",
        "\n",
        "            output = model(input_seq)\n",
        "            last_word_logits = output[0, -1, :]\n",
        "\n",
        "            scaled_logits = last_word_logits / temperature\n",
        "\n",
        "            probs = F.softmax(scaled_logits, dim=0).cpu().numpy()\n",
        "\n",
        "            # sample next word\n",
        "            next_word_idx = np.random.choice(len(probs), p=probs)\n",
        "            next_word = vocab[next_word_idx]\n",
        "            generated_sequence.append(next_word)\n",
        "\n",
        "            # stop if we generate an <EOS> token\n",
        "            if next_word == '<EOS>':\n",
        "                break\n",
        "\n",
        "            # update current sequence\n",
        "            current_sequence = current_sequence[1:] + [next_word]\n",
        "\n",
        "    if generated_sequence[-1] == '<EOS>':\n",
        "        generated_sequence = generated_sequence[:-1]\n",
        "\n",
        "    return ' '.join(generated_sequence)\n",
        "\n",
        "prompt = preprocess(\"Once upon a time\")\n",
        "output = generate_text(model, prompt, max_length=50)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LumfRdYjFx8B"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "destination_dir = '/content/drive/My Drive/model_checkpoints'\n",
        "if not os.path.exists(destination_dir):\n",
        "    os.makedirs(destination_dir)\n",
        "\n",
        "\n",
        "source_dir = '/content/model_checkpoints'\n",
        "destination_dir = '/content/drive/My Drive/model_checkpoints'\n",
        "\n",
        "# Copy all files from source to destination\n",
        "for filename in os.listdir(source_dir):\n",
        "    source_file = os.path.join(source_dir, filename)\n",
        "    destination_file = os.path.join(destination_dir, filename)\n",
        "    shutil.copy2(source_file, destination_file)\n",
        "    print(f\"Copied {filename} to Google Drive\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
